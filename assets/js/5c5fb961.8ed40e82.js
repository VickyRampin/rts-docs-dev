"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["5735"],{28852(e,n,t){t.r(n),t.d(n,{metadata:()=>a,default:()=>l,frontMatter:()=>i,contentTitle:()=>s,toc:()=>h,assets:()=>d});var a=JSON.parse('{"id":"genai/how_to_guides/retrieval_augmented_generation","title":"Retrieval-augmented generation","description":"For an in-depth overview of RAG and Jupyter notebook examples, please access the source materials used for the 2025 FORC session on RAG at//github.com/NYU-RTS/rag-forc-2025","source":"@site/docs/genai/04_how_to_guides/03_retrieval_augmented_generation.mdx","sourceDirName":"genai/04_how_to_guides","slug":"/genai/how_to_guides/retrieval_augmented_generation","permalink":"/docs/genai/how_to_guides/retrieval_augmented_generation","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/genai/04_how_to_guides/03_retrieval_augmented_generation.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"genaiSidebar","previous":{"title":"Generating embeddings","permalink":"/docs/genai/how_to_guides/embeddings"},"next":{"title":"Fine tuning","permalink":"/docs/genai/how_to_guides/llm_fine_tuning"}}'),o=t(62615),r=t(30416);let i={},s="Retrieval-augmented generation",d={},h=[];function c(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"retrieval-augmented-generation",children:"Retrieval-augmented generation"})}),"\n",(0,o.jsx)(n.admonition,{title:"Foundations of Research Computing session on RAG",type:"tip",children:(0,o.jsxs)(n.p,{children:["For an in-depth overview of RAG and Jupyter notebook examples, please access the source materials used for the 2025 FORC session on RAG at: ",(0,o.jsx)(n.a,{href:"https://github.com/NYU-RTS/rag-forc-2025",children:"https://github.com/NYU-RTS/rag-forc-2025"})]})}),"\n",(0,o.jsx)(n.p,{children:"Large Language Models only know about the data they were trained upon and do not have the context needed to be effective at answering questions based on:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"private datasets"}),"\n",(0,o.jsx)(n.li,{children:"newer knowledge past the cutoff date (i.e. the date at which data collection was frozen)"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"To get around this issue, one of the most popular techniques is Retrieval-augmented generation, the most basic version of which is outlined below:"}),"\n",(0,o.jsx)(n.mermaid,{value:'flowchart TB;\n    A["Documents to use in RAG pipeline"]\n    B@{shape: docs, label: "Document parsed and divided into multiple chunks"}\n    C["encoder-only LLM"]\n    D@{shape: procs, label: "text chunk embedding"}\n    E[("vector database")]\n    F["natual language prompt"]\n    G["query embedding"]\n    I["relevant chunks"]\n    J["original prompt with added context"]\n    K["relevant response from LLM using context"]\n    subgraph Ingestion\n    A-- "Parse and Chunk" --\x3eB;\n    end\n    subgraph Embeddings\n    B-- "Pass to embedding model" --\x3eC;\n    C-- "Generate Embeddings" --\x3eD;\n    D-- "Store Emebddings" --\x3eE;\n    end\n    subgraph Retrieval\n    F-- "Pass to embedding model" --\x3eC;\n    C-- "Generate Emebddings" --\x3eG;\n    G-- "Query vector database" --\x3eE;\n    E-- "Gather text chunks with embeddings similar to prompt" --\x3eI;\n    end\n    I-- "With expanded prompt" --\x3eJ;\n    subgraph Augmented Generation\n    J-- "LLM" --\x3eK;\n    end'}),"\n",(0,o.jsxs)(n.p,{children:['It starts with the "Ingestion" phase where a document to be used as context is parsed and broken into chunks. These chunks are then converted to embeddings and stored in a vector database (which specializes in storing and retrieving vectors). This setup allows us now "retrieve" the required context for an incoming prompt before it is sent to an LLM. The "retrieval" phase consists of converting the prompt to an embedding and looking up embeddings for chunks of the document that are similar to it. The text chunks associated with the embeddings similar to the embedding for the query are then added as additional context to the prompt before passing it to an LLM.\nThe LLM now has the associated context it needs to generate an relevant response to the prompt. Here\'s a link to a script to test this out yourself, once you have API access to an embedding model and an LLM: ',(0,o.jsx)(n.a,{href:"https://github.com/NYU-RTS/rts-docs-examples/tree/main/genai/rag",children:"https://github.com/NYU-RTS/rts-docs-examples/tree/main/genai/rag"})," ."]}),"\n",(0,o.jsx)(n.p,{children:"You can run it to ask a question about a recent event that occurred after the knowledge cutoff for the dataset used to train the LLM:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"ss19980@ITS-JQKQGQQMTX ~/D/p/r/g/rag (main)> uv run rag_basic.py \\\n                                                 https://en.wikipedia.org/wiki/2025_London_Marathon \\\n                                                 \"Which athletes won the 2025 London Marathon?\"\n...\nProcessing chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44/44 [00:13<00:00,  3.22it/s]\n----------------------------------------------------------------\nQuery embedding vector (first 5 dims) is:  [-0.013783585280179977, 0.022411219775676727, -0.018617955967783928, -0.04355597868561745, -0.009368936531245708]\n----------------------------------------------------------------\nRetrieved chunks and similarity scores:\n\n('The 2025 London Marathon was the 45th running of the London Marathon; it took place on 27 April 2025.[1][2]', 0.8441829085350037)\n('1. ^ Martin, Andy (26 April 2025). \"London Marathon 2025: route, runners and everything else you need to know\". The Guardian. Retrieved 26 April 2025.\\n2. ^ Poole, Harry (26 April 2025). \"Will \\'greatest\\' London Marathon line-ups break records?\". BBC News. Retrieved 26 April 2025.\\n3. ^ a b c d \"2025 London Marathon results\". NBC Sports. 27 April 2025. Retrieved 27 April 2025.', 0.837587296962738)\n('Venue, 45th London Marathon = London, England. Date, 45th London Marathon = 27 April 2025. Champions, 45th London Marathon = Champions. Men, 45th London Marathon = Sabastian Sawe (2:02:27). Women, 45th London Marathon = Tigst Assefa (2:15:50). Wheelchair men, 45th London Marathon = Marcel Hug (1:25:25). Wheelchair women, 45th London Marathon = Catherine Debrunner (1:34:18). \u2190\\xa020242026\\xa0\u2192, 45th London Marathon = \u2190\\xa020242026\\xa0\u2192', 0.8032743334770203)\n----------------------------------------------------------------\nGenerated response from LLM without additional context is:\n\nThe 2025 London Marathon has not happened yet!\n\nThe event typically takes place in **April** each year. We will only know the winners after the race takes place in April 2025.\n---------------------------------------------------------------\nGenerated response from LLM with additional context is:\n\nThe athletes who won the 2025 London Marathon are:\n\n*   **Men:** Sabastian Sawe\n*   **Women:** Tigst Assefa\n*   **Wheelchair men:** Marcel Hug\n*   **Wheelchair women:** Catherine Debrunner\nE20250611 14:32:54.595919 362220 server.cpp:47] [SERVER][BlockLock][] Process exit\nss19980@ITS-JQKQGQQMTX ~/D/p/r/g/rag (main)>\n"})})]})}function l(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},30416(e,n,t){t.d(n,{R:()=>i,x:()=>s});var a=t(59471);let o={},r=a.createContext(o);function i(e){let n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);