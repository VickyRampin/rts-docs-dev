"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["9857"],{9843(e,n,o){o.r(n),o.d(n,{metadata:()=>l,default:()=>d,frontMatter:()=>a,contentTitle:()=>r,toc:()=>c,assets:()=>i});var l=JSON.parse('{"id":"hpc/tutorial_intro_hpc/running_parallel_job","title":"Running a parallel job","description":"Questions","source":"@site/docs/hpc/13_tutorial_intro_hpc/08_running_parallel_job.mdx","sourceDirName":"hpc/13_tutorial_intro_hpc","slug":"/hpc/tutorial_intro_hpc/running_parallel_job","permalink":"/docs/hpc/tutorial_intro_hpc/running_parallel_job","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/13_tutorial_intro_hpc/08_running_parallel_job.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Transferring files with remote computers","permalink":"/docs/hpc/tutorial_intro_hpc/transferring_files_remote"},"next":{"title":"Using resources effectively","permalink":"/docs/hpc/tutorial_intro_hpc/using_resources_effectively"}}'),s=o(62615),t=o(30416);let a={},r="Running a parallel job",i={},c=[{value:"Install the Amdahl Program",id:"install-the-amdahl-program",level:2},{value:"Help!",id:"help",level:2},{value:"Running the Job on a Compute Node",id:"running-the-job-on-a-compute-node",level:2},{value:"Running the Parallel Job",id:"running-the-parallel-job",level:2},{value:"How Much Does Parallel Execution Improve Performance?",id:"how-much-does-parallel-execution-improve-performance",level:2}];function h(e){let n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"running-a-parallel-job",children:"Running a parallel job"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Overview",type:"info",children:[(0,s.jsx)(n.p,{children:"Questions"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How do we execute a task in parallel?"}),"\n",(0,s.jsx)(n.li,{children:"What benefits arise from parallel execution?"}),"\n",(0,s.jsx)(n.li,{children:"What are the limits of gains from execution in parallel?"}),"\n"]}),(0,s.jsx)(n.p,{children:"Objectives"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Install a Python package using ",(0,s.jsx)(n.code,{children:"pip"})]}),"\n",(0,s.jsx)(n.li,{children:"Prepare a job submission script for the parallel executable."}),"\n",(0,s.jsx)(n.li,{children:"Launch jobs with parallel execution."}),"\n",(0,s.jsx)(n.li,{children:"Record and summarize the timing and accuracy of jobs."}),"\n",(0,s.jsx)(n.li,{children:"Describe the relationship between job parallelism and performance."}),"\n"]})]}),"\n",(0,s.jsx)(n.p,{children:"We now have the tools we need to run a multi-processor job. This is a very important aspect of HPC systems, as parallelism is one of the primary tools we have to improve the performance of computational tasks."}),"\n",(0,s.jsx)(n.p,{children:"If you disconnected, log back in to the cluster."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[user@laptop ~]$ ssh NetID@login.torch.hpc.nyu.edu\n"})}),"\n",(0,s.jsx)(n.h2,{id:"install-the-amdahl-program",children:"Install the Amdahl Program"}),"\n",(0,s.jsx)(n.p,{children:"With the Amdahl source code on the cluster, we can install it, which will provide access to the amdahl executable."}),"\n",(0,s.jsxs)(n.p,{children:["The Amdahl code has one dependency: ",(0,s.jsx)(n.code,{children:"mpi4py"}),". Package Installer for Python (pip) will collect ",(0,s.jsx)(n.code,{children:"mpi4py"})," from the Internet and install it for you, but it needs an active ",(0,s.jsx)(n.code,{children:"mpi"})," module. Here we will load ",(0,s.jsx)(n.code,{children:"openmpi/gcc/4.1.6"})," to build ",(0,s.jsx)(n.code,{children:"mpi4py"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Let's start by moving the ",(0,s.jsx)(n.code,{children:"amdahl"})," directory form our home directory to our scratch directory to save our valuable home directory space:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 ~]$ mv amdahl /scratch/$USER\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["For more information about storage options on Torch please see ",(0,s.jsx)(n.a,{href:"/docs/hpc/storage/intro_and_data_management",children:"HPC Storage"}),"."]})}),"\n",(0,s.jsx)(n.p,{children:"Move into the extracted directory, then use pip, to install it in your (\u201Cuser\u201D) scratch directory:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 ~]$ cd /scratch/$USER/amdahl\n[NetID@log-1 amdahl]$ module load python/intel/3.8.6\n[NetID@log-1 amdahl]$ python -m venv ./test_venv\n[NetID@log-1 amdahl]$ source ./test_venv/bin/activate\n[NetID@log-1 amdahl]$ module load openmpi/gcc/4.1.6\n[NetID@log-1 amdahl]$ python -m pip install .\nProcessing /scratch/NetID/packages/temp/hpc-carpentry-amdahl-46c9b4b\nCollecting mpi4py\n  Using cached mpi4py-4.0.0.tar.gz (464 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n    Preparing wheel metadata ... done\nBuilding wheels for collected packages: amdahl, mpi4py\n  Building wheel for amdahl (setup.py) ... done\n  Created wheel for amdahl: filename=amdahl-0.3.1-py3-none-any.whl size=6996 sha256=13a95c3e6fbc53fde1c90a4a9bbb3fd3179d5e3afa3e19b4131a05d9ac798981\n  Stored in directory: /scratch/NetID/.cache/pip/wheels/2c/53/fc/19c3053b3a1d3625ac26158b28f263783f66ec258df97aefcf\n  Building wheel for mpi4py (PEP 517) ... done\n  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp38-cp38-linux_x86_64.whl size=5169079 sha256=9afceb56e22608a7de33442a60bbde3cbd4aa06947d48de5f6dc63932d34bc9f\n  Stored in directory: /scratch/NetID/.cache/pip/wheels/31/3b/6f/dc579e9ff3e2273078596b0cbc1e8d6cbf5a3a05cfad4a380a\nSuccessfully built amdahl mpi4py\nInstalling collected packages: mpi4py, amdahl\nSuccessfully installed amdahl-0.3.1 mpi4py-4.0.0\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Amdahl is Python Code",type:"note",children:(0,s.jsx)(n.p,{children:"The Amdahl program is written in Python, and installing or using it requires locating the python executable on the login node."})}),"\n",(0,s.jsx)(n.h2,{id:"help",children:"Help!"}),"\n",(0,s.jsxs)(n.p,{children:["Many command-line programs include a \u201Chelp\u201D message. Try it with ",(0,s.jsx)(n.code,{children:"amdahl"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ amdahl --help\nusage: amdahl [-h] [-p [PARALLEL_PROPORTION]] [-w [WORK_SECONDS]] [-t] [-e] [-j [JITTER_PROPORTION]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -p [PARALLEL_PROPORTION], --parallel-proportion [PARALLEL_PROPORTION]\n                        Parallel proportion: a float between 0 and 1\n  -w [WORK_SECONDS], --work-seconds [WORK_SECONDS]\n                        Total seconds of workload: an integer greater than 0\n  -t, --terse           Format output as a machine-readable object for easier analysis\n  -e, --exact           Exactly match requested timing by disabling random jitter\n  -j [JITTER_PROPORTION], --jitter-proportion [JITTER_PROPORTION]\n                        Random jitter: a float between -1 and +1\n"})}),"\n",(0,s.jsx)(n.p,{children:"This message doesn\u2019t tell us much about what the program does, but it does tell us the important flags we might want to use when launching it."}),"\n",(0,s.jsx)(n.h2,{id:"running-the-job-on-a-compute-node",children:"Running the Job on a Compute Node"}),"\n",(0,s.jsx)(n.p,{children:"Create a submission file, requesting one task on a single node, then launch it."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ nano serial-job.sh\n[NetID@log-1 amdahl]$ cat serial-job.sh\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH -J solo-job\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1 \n#SBATCH --mem=3G\n\n# Load the computing environment we need\nmodule load python/intel/3.8.6\nmodule load openmpi/gcc/4.1.6\nsource /scratch/NetID/amdahl/test_venv/bin/activate\n\n# Execute the task\nsrun amdahl\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ sbatch serial-job.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"As before, use the Slurm status commands to check whether your job is running and when it ends:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ squeue -u NetID\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"ls"})," to locate the output file. The ",(0,s.jsx)(n.code,{children:"-t"})," flag sorts in reverse-chronological order: newest first. What was the output?"]}),"\n",(0,s.jsxs)(o,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.admonition,{title:"Read the Job Output",type:"info",children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[Click for Output]"})})})}),(0,s.jsxs)(n.admonition,{title:"Output",type:"tip",children:[(0,s.jsx)(n.p,{children:"The cluster output should be written to a file in the folder you launched the job from. For example,"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ ls -t\nslurm-347087.out  serial-job.sh  amdahl  README.md  LICENSE.txt\n[NetID@log-1 amdahl]$ cat slurm-347087.out\nDoing 30.000 seconds of 'work' on 1 processor,\nwhich should take 30.000 seconds with 0.850 parallel proportion of the workload.\n\n  Hello, World! I am process 0 of 1 on cs. I will do all the serial 'work' for 4.500 seconds.\n  Hello, World! I am process 0 of 1 on cs. I will do parallel 'work' for 25.500 seconds.\n\nTotal execution time (according to rank 0): 30.033 seconds\n"})})]})]}),"\n",(0,s.jsxs)(n.p,{children:["As we saw before, two of the ",(0,s.jsx)(n.code,{children:"amdahl"})," program flags set the amount of work and the proportion of that work that is parallel in nature. Based on the output, we can see that the code uses a default of 30 seconds of work that is 85% parallel. The program ran for just over 30 seconds in total, and if we run the numbers, it is true that 15% of it was marked \u2018serial\u2019 and 85% was \u2018parallel\u2019."]}),"\n",(0,s.jsx)(n.p,{children:"Since we only gave the job one CPU, this job wasn\u2019t really parallel: the same processor performed the \u2018serial\u2019 work for 4.5 seconds, then the \u2018parallel\u2019 part for 25.5 seconds, and no time was saved. The cluster can do better, if we ask."}),"\n",(0,s.jsx)(n.h2,{id:"running-the-parallel-job",children:"Running the Parallel Job"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"amdahl"})," program uses the Message Passing Interface (MPI) for parallelism \u2013 this is a common tool on HPC systems."]}),"\n",(0,s.jsx)(n.admonition,{title:"What is MPI?",type:"tip",children:(0,s.jsx)(n.p,{children:"The Message Passing Interface is a set of tools which allow multiple tasks running simultaneously to communicate with each other. Typically, a single executable is run multiple times, possibly on different machines, and the MPI tools are used to inform each instance of the executable about its sibling processes, and which instance it is. MPI also provides tools to allow communication between instances to coordinate work, exchange information about elements of the task, or to transfer data. An MPI instance typically has its own copy of all the local variables."})}),"\n",(0,s.jsxs)(n.p,{children:["While MPI-aware executables can generally be run as stand-alone programs, in order for them to run in parallel they must use an MPI ",(0,s.jsx)(n.em,{children:"run-time environment"}),", which is a specific implementation of the MPI ",(0,s.jsx)(n.em,{children:"standard"}),". To activate the MPI environment, the program should be started via a command such as ",(0,s.jsx)(n.code,{children:"mpiexec"})," (or ",(0,s.jsx)(n.code,{children:"mpirun"}),", or ",(0,s.jsx)(n.code,{children:"srun"}),", etc. depending on the MPI run-time you need to use), which will ensure that the appropriate run-time support for parallelism is included."]}),"\n",(0,s.jsx)(n.admonition,{title:"MPI Runtime Arguments",type:"tip",children:(0,s.jsxs)(n.p,{children:["On their own, commands such as ",(0,s.jsx)(n.code,{children:"mpiexec"})," can take many arguments specifying how many machines will participate in the execution, and you might need these if you would like to run an MPI program on your own (for example, on your laptop). In the context of a queuing system, however, it is frequently the case that MPI run-time will obtain the necessary parameters from the queuing system, by examining the environment variables set when the job is launched."]})}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s modify the job script to request more cores and use the MPI run-time."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 ~]$ cp serial-job.sh parallel-job.sh\n[NetID@log-1 ~]$ nano parallel-job.sh\n[NetID@log-1 ~]$ cat parallel-job.sh\n#!/bin/bash\n#SBATCH -J parallel-pi\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=1 \n#SBATCH --mem=3G\n\n# Load the computing environment we need\nmodule load python/intel/3.8.6\nmodule load openmpi/gcc/4.1.6\nsource /home/yourUsername/amdahl/test_venv/bin/activate\n\n# Execute the task\nsrun amdahl\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then submit your job. Note that the submission command has not really changed from how we submitted the serial job: all the parallel settings are in the batch file rather than the command line."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ sbatch parallel-job.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"As before, use the status commands to check when your job runs."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ ls -t\nslurm-347178.out  parallel-job.sh  slurm-347087.out  serial-job.sh  amdahl  README.md  LICENSE.txt\n[NetID@log-1 amdahl]$ cat slurm-347178.out\nDoing 30.000 seconds of 'work' on 4 processors,\nwhich should take 10.875 seconds with 0.850 parallel proportion of the workload.\n\n  Hello, World! I am process 0 of 4 on cs. I will do all the serial 'work' for 4.500 seconds.\n  Hello, World! I am process 2 of 4 on cs. I will do parallel 'work' for 6.375 seconds.\n  Hello, World! I am process 1 of 4 on cs. I will do parallel 'work' for 6.375 seconds.\n  Hello, World! I am process 3 of 4 on cs. I will do parallel 'work' for 6.375 seconds.\n  Hello, World! I am process 0 of 4 on cs. I will do parallel 'work' for 6.375 seconds.\n\nTotal execution time (according to rank 0): 10.888 seconds\n"})}),"\n",(0,s.jsxs)(o,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.admonition,{title:"Is it 4\xd7 faster?",type:"info",children:(0,s.jsxs)(n.p,{children:["The parallel job received 4\xd7 more processors than the serial job: does that mean it finished in \xbc the time? ",(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.strong,{children:"[Click for Solution]"})]})})}),(0,s.jsxs)(n.admonition,{title:"Solution",type:"tip",children:[(0,s.jsx)(n.p,{children:"The parallel job did take less time: 11 seconds is better than 30! But it is only a 2.7\xd7 improvement, not 4\xd7."}),(0,s.jsx)(n.p,{children:"Look at the job output:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"While \u201Cprocess 0\u201D did serial work, processes 1 through 3 did their parallel work."}),"\n",(0,s.jsx)(n.li,{children:"While process 0 caught up on its parallel work, the rest did nothing at all."}),"\n"]}),(0,s.jsx)(n.p,{children:"Process 0 always has to finish its serial task before it can start on the parallel work. This sets a lower limit on the amount of time this job will take, no matter how many cores you throw at it."}),(0,s.jsxs)(n.p,{children:["This is the basic principle behind ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Amdahl's_law",children:"Amdahl\u2019s Law"}),", which is one way of predicting improvements in execution time for a fixed workload that can be subdivided and run in parallel to some extent."]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"how-much-does-parallel-execution-improve-performance",children:"How Much Does Parallel Execution Improve Performance?"}),"\n",(0,s.jsxs)(n.p,{children:["In theory, dividing up a perfectly parallel calculation among ",(0,s.jsx)(n.em,{children:"n"})," MPI processes should produce a decrease in total run time by a factor of ",(0,s.jsx)(n.em,{children:"n"}),". As we have just seen, real programs need some time for the MPI processes to communicate and coordinate, and some types of calculations can\u2019t be subdivided: they only run effectively on a single CPU."]}),"\n",(0,s.jsx)(n.p,{children:"Additionally, if the MPI processes operate on different physical CPUs in the computer, or across multiple compute nodes, even more time is required for communication than it takes when all processes operate on a single CPU."}),"\n",(0,s.jsx)(n.p,{children:"In practice, it\u2019s common to evaluate the parallelism of an MPI program by:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"running the program across a range of CPU counts"}),"\n",(0,s.jsx)(n.li,{children:"recording the execution time on each run"}),"\n",(0,s.jsx)(n.li,{children:"comparing each execution time to the time when using a single CPU"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Since \u201Cmore is better\u201D \u2013 improvement is easier to interpret from increases in some quantity than decreases \u2013 comparisons are made using the speedup factor ",(0,s.jsx)(n.em,{children:"S"}),", which is calculated as the single-CPU execution time divided by the multi-CPU execution time. For a perfectly parallel program, a plot of the speedup S versus the number of CPUs ",(0,s.jsx)(n.em,{children:"n"})," would give a straight line, S = n."]}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s run one more job, so we can see how close to a straight line our amdahl code gets."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ nano parallel-job.sh\n[NetID@log-1 amdahl]$ cat parallel-job.sh\n#!/bin/bash\n#SBATCH -J parallel-pi\n#SBATCH --nodes=8\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem=3G\n\n# Load the computing environment we need\nmodule load python/intel/3.8.6\nmodule load openmpi/gcc/4.1.6\nsource /home/yourUsername/amdahl/test_venv/bin/activate\n\n# Execute the task\nsrun amdahl\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then submit your job. Note that the submission command has not really changed from how we submitted the serial job: all the parallel settings are in the batch file rather than the command line."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ sbatch parallel-job.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"As before, use the status commands to check when your job runs."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 amdahl]$ ls -t\nslurm-347271.out  parallel-job.sh  slurm-347178.out  slurm-347087.out  serial-job.sh  amdahl  README.md  LICENSE.txt\n[NetID@log-1 amdahl]$ cat slurm-347178.out\nwhich should take 7.688 seconds with 0.850 parallel proportion of the workload.\n\n  Hello, World! I am process 4 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 0 of 8 on cs. I will do all the serial 'work' for 4.500 seconds.\n  Hello, World! I am process 2 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 1 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 3 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 5 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 6 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 7 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n  Hello, World! I am process 0 of 8 on cs. I will do parallel 'work' for 3.188 seconds.\n\nTotal execution time (according to rank 0): 7.697 seconds\n"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Non-Linear Output",type:"info",children:[(0,s.jsx)(n.p,{children:"When we ran the job with 4 parallel workers, the serial job wrote its output first, then the parallel processes wrote their output, with process 0 coming in first and last."}),(0,s.jsx)(n.p,{children:"With 8 workers, this is not the case: since the parallel workers take less time than the serial work, it is hard to say which process will write its output first, except that it will not be process 0!"})]}),"\n",(0,s.jsx)(n.p,{children:"Now, let\u2019s summarize the amount of time it took each job to run:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Number of CPUs"}),(0,s.jsx)(n.th,{children:"Runtime (sec)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"30.033"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:"10.888"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"7.697"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Then, use the first row to compute speedups S, using Python as a command-line calculator:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'[NetID@log-1 amdahl]$ for n in 30.033 10.888 7.697; do python3 -c "print(30.033 / $n)"; done\n'})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Number of CPUs"}),(0,s.jsx)(n.th,{children:"Speedup"}),(0,s.jsx)(n.th,{children:"Ideal"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"1.0"}),(0,s.jsx)(n.td,{children:"1"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:"2.75"}),(0,s.jsx)(n.td,{children:"4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"3.90"}),(0,s.jsx)(n.td,{children:"8"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The job output files have been telling us that this program is performing 85% of its work in parallel, leaving 15% to run in serial. This seems reasonably high, but our quick study of speedup shows that in order to get a 4\xd7 speedup, we have to use 8 or 9 processors in parallel. In real programs, the speedup factor is influenced by"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"CPU design"}),"\n",(0,s.jsx)(n.li,{children:"communication network between compute nodes"}),"\n",(0,s.jsx)(n.li,{children:"MPI library implementations"}),"\n",(0,s.jsx)(n.li,{children:"details of the MPI program itself"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Using Amdahl\u2019s Law, you can prove that with this program, it is ",(0,s.jsx)(n.em,{children:"impossible"})," to reach 8\xd7 speedup, no matter how many processors you have on hand.  We'll discuss estimating job resources more in the next tutorial ",(0,s.jsx)(n.a,{href:"/docs/hpc/tutorial_intro_hpc/using_resources_effectively",children:"Using Resources Effectively"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["In an HPC environment, we try to reduce the execution time for all types of jobs, and MPI is an extremely common way to combine dozens, hundreds, or thousands of CPUs into solving a single problem. To learn more about parallelization, see the ",(0,s.jsx)(n.a,{href:"https://www.hpc-carpentry.org/hpc-parallel-novice/",children:"parallel novice lesson"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Key Points"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Parallel programming allows applications to take advantage of parallel hardware."}),"\n",(0,s.jsx)(n.li,{children:"The queuing system facilitates executing parallel tasks."}),"\n",(0,s.jsx)(n.li,{children:"Performance improvements from parallel execution do not scale linearly."}),"\n"]})]})}function d(e={}){let{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},30416(e,n,o){o.d(n,{R:()=>a,x:()=>r});var l=o(59471);let s={},t=l.createContext(s);function a(e){let n=l.useContext(t);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),l.createElement(t.Provider,{value:n},e.children)}}}]);