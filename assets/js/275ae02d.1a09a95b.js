"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["2994"],{27572(e,n,i){i.r(n),i.d(n,{metadata:()=>t,default:()=>h,frontMatter:()=>o,contentTitle:()=>l,toc:()=>s,assets:()=>c});var t=JSON.parse('{"id":"hpc/ml_ai_hpc/LLM Inference/run_hf_model","title":"Run a Hugging Face model","description":"Here we provide an example of how one can run a Hugging Face Large-language model (LLM) on the NYU Torch cluster","source":"@site/docs/hpc/08_ml_ai_hpc/LLM Inference/05_run_hf_model.md","sourceDirName":"hpc/08_ml_ai_hpc/LLM Inference","slug":"/hpc/ml_ai_hpc/LLM Inference/run_hf_model","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/run_hf_model","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/LLM Inference/05_run_hf_model.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Fine tune LLMs on HPC","permalink":"/docs/hpc/ml_ai_hpc/llm_fine_tuning"},"next":{"title":"vLLM - A Command Line LLM Tool","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/vLLM"}}'),r=i(62615),a=i(30416);let o={},l="Run a Hugging Face model",c={},s=[{value:"Prepare environment",id:"prepare-environment",level:2},{value:"Create project directory",id:"create-project-directory",level:3},{value:"Move to a compute node",id:"move-to-a-compute-node",level:3},{value:"Copy appropriate overlay file to the project directory",id:"copy-appropriate-overlay-file-to-the-project-directory",level:3},{value:"Launch Singularity container in read/write mode",id:"launch-singularity-container-in-readwrite-mode",level:3},{value:"Install miniconda in the container",id:"install-miniconda-in-the-container",level:3},{value:"Create environment script",id:"create-environment-script",level:3},{value:"Activate the environment",id:"activate-the-environment",level:3},{value:"Install packages in environment",id:"install-packages-in-environment",level:3},{value:"Exit from Singularity and the compute node",id:"exit-from-singularity-and-the-compute-node",level:3},{value:"Prepare script",id:"prepare-script",level:2},{value:"Prepare Sbatch file",id:"prepare-sbatch-file",level:2},{value:"Run the run.SBATCH file",id:"run-the-runsbatch-file",level:2},{value:"Acknowledgements",id:"acknowledgements",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"run-a-hugging-face-model",children:"Run a Hugging Face model"})}),"\n",(0,r.jsx)(n.p,{children:"Here we provide an example of how one can run a Hugging Face Large-language model (LLM) on the NYU Torch cluster"}),"\n",(0,r.jsx)(n.h2,{id:"prepare-environment",children:"Prepare environment"}),"\n",(0,r.jsx)(n.h3,{id:"create-project-directory",children:"Create project directory"}),"\n",(0,r.jsxs)(n.p,{children:["After ",(0,r.jsx)(n.a,{href:"/docs/hpc/connecting_to_hpc/connecting_to_hpc",children:"logging on to a Torch login node"}),", make a directory for this project:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 ~]$ mkdir -p /scratch/NetID/llm_example\n[NetID@log-1 ~]$ cd /scratch/NetID/llm_example\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"You'll need to replace NetID above with your NetID"})}),"\n",(0,r.jsx)(n.h3,{id:"move-to-a-compute-node",children:"Move to a compute node"}),"\n",(0,r.jsx)(n.p,{children:"Some of the following steps can require significant resources, so we'll move to a compute node.  This way we won't overload the login node we're on."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 llm_example]$ srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash\n"})}),"\n",(0,r.jsx)(n.h3,{id:"copy-appropriate-overlay-file-to-the-project-directory",children:"Copy appropriate overlay file to the project directory"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"[NetID@cm001 llm_example]$ cp -rp /share/apps/overlay-fs-ext3/overlay-50G-10M.ext3.gz .\n[NetID@cm001 llm_example]$ gunzip overlay-50G-10M.ext3.gz\n"})}),"\n",(0,r.jsx)(n.h3,{id:"launch-singularity-container-in-readwrite-mode",children:"Launch Singularity container in read/write mode"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"[NetID@cm001 llm_example]$ apptainer exec --fakeroot --overlay overlay-50G-10M.ext3:rw /share/apps/images/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash\n"})}),"\n",(0,r.jsx)(n.h3,{id:"install-miniconda-in-the-container",children:"Install miniconda in the container"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Singularity> wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nSingularity> bash Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3\n"})}),"\n",(0,r.jsx)(n.h3,{id:"create-environment-script",children:"Create environment script"}),"\n",(0,r.jsxs)(n.p,{children:["Use an editor like nano or vim to create the file ",(0,r.jsx)(n.code,{children:"/ext3/env.sh"}),".  The contents should be:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n\nunset -f which\n\nsource /ext3/miniforge3/etc/profile.d/conda.sh\nexport PATH=/ext3/miniforge3/bin:$PATH\nexport PYTHONPATH=/ext3/miniforge3/bin:$PATH\n"})}),"\n",(0,r.jsx)(n.h3,{id:"activate-the-environment",children:"Activate the environment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Singularity> source /ext3/env.sh\n"})}),"\n",(0,r.jsx)(n.h3,{id:"install-packages-in-environment",children:"Install packages in environment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Singularity> conda config --remove channels defaults\nSingularity> conda update -n base conda -y\nSingularity> conda clean --all --yes\nSingularity> conda install pip -y\nSingularity> pip install torch numpy transformers\n"})}),"\n",(0,r.jsx)(n.h3,{id:"exit-from-singularity-and-the-compute-node",children:"Exit from Singularity and the compute node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Singularity> exit\n[NetID@cm001 llm_example]$ exit\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["You can find more information about using Singularity and Conda on our HPC systems in our documentation ",(0,r.jsx)(n.a,{href:"https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/torch/software/singularity-with-miniconda",children:"Singularity with Conda"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"prepare-script",children:"Prepare script"}),"\n",(0,r.jsxs)(n.p,{children:["Create a python script using the following code from sections 1-9 and save it in a file called ",(0,r.jsx)(n.code,{children:"huggingface.py"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Import necessary modules:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Create a list of reviews:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'texts = ["How do I get a replacement Medicare card?",\n        	"What is the monthly premium for Medicare Part B?",\n        	"How do I terminate my Medicare Part B (medical insurance)?",\n		    "How do I sign up for Medicare?",\n		    "Can I sign up for Medicare Part B if I am working and have health insurance through an employer?",\n       		"How do I sign up for Medicare Part B if I already have Part A?"]\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Choose the model name from huggingface\u2019s model hub and instantiate the model and tokenizer object for the given model. We are setting ",(0,r.jsx)(n.code,{children:"output_hidden_states"})," as ",(0,r.jsx)(n.code,{children:"True"})," as we want the output of the model to not only have loss, but also the embeddings for the sentences."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Create the ids to be used in the model using the tokenizer object. We set the return_tensors as \u201Cpt\u201D as we want to return the pytorch tensor of the ids:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'ids = tokenizer(texts, padding=True, return_tensors="pt")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Set the device to cuda, and move the model and the tokenizer to cuda as well. Since, we will be extracting embeddings, we will only be performing a forward pass of the model and hence we will set the model to validation mode using ",(0,r.jsx)(n.code,{children:"eval()"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\nids = ids.to(device)	\nmodel.eval()\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Performing the forward pass and storing the output tuple in out:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"with torch.no_grad():\n    out = model(**ids)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Extracting the embeddings of each review from the last layer:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"last_hidden_states = out.last_hidden_state	\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"For the purpose of classification, we are extracting the CLS token which is the first embedding in the embedding list for each review:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"sentence_embedding = last_hidden_states[:, 0, :]\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["We can check the shape of the final sentence embeddings for all the reviews. The output should look like ",(0,r.jsx)(n.code,{children:"torch.Size([6, 768])"}),", where 6 is the batch size as we input 6 reviews as shown in step ",(0,r.jsx)(n.code,{children:"2b"}),", and 768 is the embedding size of the RoBERTa model used."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("Shape of the batch embedding: {}".format(sentence_embedding.shape))\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prepare-sbatch-file",children:"Prepare Sbatch file"}),"\n",(0,r.jsxs)(n.p,{children:["After saving the above code in a script called ",(0,r.jsx)(n.code,{children:"huggingface.py"}),", create a file called ",(0,r.jsx)(n.code,{children:"run.SBATCH"})," with the the following code:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-batch",children:'#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:10:00\n#SBATCH --mem=64GB\n#SBATCH --gres=gpu\n#SBATCH --job-name=huggingface\n#SBATCH --output=huggingface.out\n\nmodule purge\n\nif [ -e /dev/nvidia0 ]; then nv="--nv"; fi\n\nsingularity exec $nv \\\n  --overlay /scratch/NetID/llm_example/overlay-50G-10M.ext3:rw \\\n  /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif \\\n  /bin/bash -c "source /ext3/env.sh; python /scratch/NetID/llm_example/huggingface.py"\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["You'll need to change ",(0,r.jsx)(n.code,{children:"NetID"})," in the script above to your NetID.\nIf you're using a different directory name and/or path you'll also need to update that in the script above."]})}),"\n",(0,r.jsx)(n.h2,{id:"run-the-runsbatch-file",children:"Run the run.SBATCH file"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-batch",children:"[NetID@log-1 llm_example]$ sbatch run.SBATCH\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The output can be found in ",(0,r.jsx)(n.code,{children:"huggingface.out"}),"\nIt should be something like:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are\n newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nShape of the batch embedding: torch.Size([6, 768])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"acknowledgements",children:"Acknowledgements"}),"\n",(0,r.jsxs)(n.p,{children:["Instructions are developed and provided by ",(0,r.jsx)(n.a,{href:"https://www.linkedin.com/in/laiba-mehnaz-a81455158/",children:"Laiba Mehnaz"}),", a member of ",(0,r.jsx)(n.a,{href:"https://www.linkedin.com/company/ai-for-scientific-research",children:"AIfSR"})]})]})}function h(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},30416(e,n,i){i.d(n,{R:()=>o,x:()=>l});var t=i(59471);let r={},a=t.createContext(r);function o(e){let n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);