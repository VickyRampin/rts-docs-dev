"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["8519"],{33977(e,n,r){r.r(n),r.d(n,{metadata:()=>i,default:()=>h,frontMatter:()=>s,contentTitle:()=>a,toc:()=>l,assets:()=>c});var i=JSON.parse('{"id":"hpc/ml_ai_hpc/LLM Inference/llm_inferenceoverview","title":"Overview","description":"This directory provides two primary pathways for deploying and running Large Language Models (LLMs) on the NYU Torch/Greene cluster: Hugging Face Transformers (for research/experimentation) and vLLM (for high-performance serving).","source":"@site/docs/hpc/08_ml_ai_hpc/LLM Inference/llm_inferenceoverview.md","sourceDirName":"hpc/08_ml_ai_hpc/LLM Inference","slug":"/hpc/ml_ai_hpc/LLM Inference/llm_inferenceoverview","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/llm_inferenceoverview","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/LLM Inference/llm_inferenceoverview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"vLLM - A Command Line LLM Tool","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/vLLM"},"next":{"title":"Introduction to Open OnDemand (OOD)","permalink":"/docs/hpc/ood/ood_intro"}}'),t=r(62615),o=r(30416);let s={},a="Overview",c={},l=[{value:"1. Basic Inference (Hugging Face)",id:"1-basic-inference-hugging-face",level:2},{value:"2. High-Performance Serving (vLLM)",id:"2-high-performance-serving-vllm",level:2}];function d(e){let n={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,t.jsx)(n.p,{children:"This directory provides two primary pathways for deploying and running Large Language Models (LLMs) on the NYU Torch/Greene cluster: Hugging Face Transformers (for research/experimentation) and vLLM (for high-performance serving)."}),"\n",(0,t.jsx)(n.h2,{id:"1-basic-inference-hugging-face",children:"1. Basic Inference (Hugging Face)"}),"\n",(0,t.jsx)(n.p,{children:"This method is ideal for feature extraction, embeddings, or small-scale batch processing. It relies on a persistent Singularity environment using an .ext3 overlay."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Workflow:"}),"\nEnvironment: Launch an Apptainer container with a read/write overlay.\nPersistence: Install Conda/Miniforge and libraries directly into /ext3.\nExecution: Use AutoModel to load weights and perform a forward pass.\nKey File: huggingface.py\nIdeal for: Extracting last_hidden_state (embeddings) or sentiment classification."]}),"\n",(0,t.jsx)(n.h2,{id:"2-high-performance-serving-vllm",children:"2. High-Performance Serving (vLLM)"}),"\n",(0,t.jsx)(n.p,{children:"vLLM is the recommended tool for production-level throughput and low-latency inference. It utilizes PagedAttention to manage memory efficiently."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why vLLM?"}),"\nSpeed: higher throughput than standard backends on Torch.\nCompatibility: Drop-in replacement for OpenAI API."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deployment Options:"}),"\nOnline: Use vllm serve to start an HTTP server accessible via curl or OpenAI clients.\nOffline: Use the LLM class within Python for processing large datasets without a server."]})]})}function h(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},30416(e,n,r){r.d(n,{R:()=>s,x:()=>a});var i=r(59471);let t={},o=i.createContext(t);function s(e){let n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);