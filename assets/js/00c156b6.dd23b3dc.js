"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["6959"],{65090(e,n,l){l.r(n),l.d(n,{metadata:()=>t,default:()=>d,frontMatter:()=>s,contentTitle:()=>o,toc:()=>a,assets:()=>c});var t=JSON.parse('{"id":"hpc/ml_ai_hpc/LLM Inference/vLLM","title":"vLLM - A Command Line LLM Tool","description":"What is vLLM?","source":"@site/docs/hpc/08_ml_ai_hpc/LLM Inference/07_vLLM.md","sourceDirName":"hpc/08_ml_ai_hpc/LLM Inference","slug":"/hpc/ml_ai_hpc/LLM Inference/vLLM","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/vLLM","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/LLM Inference/07_vLLM.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Run a Hugging Face model","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/run_hf_model"},"next":{"title":"Overview","permalink":"/docs/hpc/ml_ai_hpc/LLM Inference/llm_inferenceoverview"}}'),r=l(62615),i=l(30416);let s={},o="vLLM - A Command Line LLM Tool",c={},a=[{value:"What is vLLM?",id:"what-is-vllm",level:2},{value:"Why vLLM?",id:"why-vllm",level:2},{value:"vLLM Installation Instructions",id:"vllm-installation-instructions",level:2},{value:"Use High-Performance SCRATCH Storage",id:"use-high-performance-scratch-storage",level:3},{value:"Run vLLM",id:"run-vllm",level:2},{value:"Online Serving (OpenAI-Compatible API)",id:"online-serving-openai-compatible-api",level:3},{value:"Offline Inference",id:"offline-inference",level:3},{value:"SGLang: A Simple Option for Offline Batch Inference (Supplement Material)",id:"sglang-a-simple-option-for-offline-batch-inference-supplement-material",level:3},{value:"vLLM CLI",id:"vllm-cli",level:2}];function h(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vllm---a-command-line-llm-tool",children:"vLLM - A Command Line LLM Tool"})}),"\n",(0,r.jsx)(n.h2,{id:"what-is-vllm",children:"What is vLLM?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM"})," is a fast and easy-to-use library for LLM inference and serving."]}),"\n",(0,r.jsx)(n.h2,{id:"why-vllm",children:"Why vLLM?"}),"\n",(0,r.jsx)(n.p,{children:"We tested vLLM and llama-cpp on Torch, and found vLLM performs better on Torch:\nModel: Qwen2.5-7B-Instruct\nPrompt Tokens:512\nOutput Tokens: 256"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Backend"}),(0,r.jsx)(n.th,{children:"Peak Throughput"}),(0,r.jsx)(n.th,{children:"Median Latency(ms)"}),(0,r.jsx)(n.th,{children:"Recommendation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"vLLM"}),(0,r.jsx)(n.td,{children:"~4689.6"}),(0,r.jsx)(n.td,{children:"~48.0"}),(0,r.jsx)(n.td,{children:"Best for Batch/Research"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"llama-cpp"}),(0,r.jsx)(n.td,{children:"~115.0"}),(0,r.jsx)(n.td,{children:"~280.0"}),(0,r.jsx)(n.td,{children:"Best for Single User"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"vllm-installation-instructions",children:"vLLM Installation Instructions"}),"\n",(0,r.jsx)(n.p,{children:"Create a vLLM directory in your /scratch directory, then install the vLLM image:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"apptainer pull docker://vllm/vllm-openai:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"use-high-performance-scratch-storage",children:"Use High-Performance SCRATCH Storage"}),"\n",(0,r.jsx)(n.p,{children:"LLMs require very fast storage. On Torch, the SCRATCH filesystem is an all-flash system designed for AI workloads, providing excellent performance.To avoid exceeding your $HOME quota (50GB) and inode limits (30,000 files), you should redirect vLLM's cache and Hugging Face's model downloads to your scratch space:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"export HF_HOME=/scratch/$USER/hf_cache\nexport VLLM_CACHE_ROOT=/scratch/$USER/vllm_cache\n"})}),"\n",(0,r.jsx)(n.p,{children:"You should run this to configure vLLM to always use your SCRATCH storage for consistent use:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'echo "export HF_HOME=/scratch/\\$USER/hf_cache" >> ~/.bashrc\necho "export VLLM_CACHE_ROOT=/scratch/\\$USER/vllm_cache" >> ~/.bashrc\n'})}),"\n",(0,r.jsx)(n.p,{children:"Note: Files on $SCRATCH are not backed up and will be deleted after 60 days of inactivity. Always keep your source code and .slurm scripts in $HOME!"}),"\n",(0,r.jsx)(n.h2,{id:"run-vllm",children:"Run vLLM"}),"\n",(0,r.jsx)(n.h3,{id:"online-serving-openai-compatible-api",children:"Online Serving (OpenAI-Compatible API)"}),"\n",(0,r.jsxs)(n.p,{children:["vLLM implements the OpenAI API protocol, allowing it to be a drop-in replacement for applications using OpenAI's services. By default, it starts the server at ",(0,r.jsx)(n.a,{href:"http://localhost:8000",children:"http://localhost:8000"}),". You can specify the address with --host and --port arguments.\n",(0,r.jsx)(n.strong,{children:"In Terminal 1:"}),"\nStart  vLLM server (In this example we use Qwen model):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'apptainer exec --nv vllm-openai_latest.sif vllm serve "Qwen/Qwen2.5-0.5B-Instruct"\n'})}),"\n",(0,r.jsx)(n.p,{children:"When you see:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Application startup complete.\n"})}),"\n",(0,r.jsx)(n.p,{children:"Open another terminal and log in to the same computing node as in terminal 1."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"In Terminal 2"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'curl http://localhost:8000/v1/chat/completions \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n        "model": "Qwen/Qwen2.5-0.5B-Instruct",\n        "messages": [\n            {"role": "user", "content": "Your prompt..."}\n        ]\n    }\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"offline-inference",children:"Offline Inference"}),"\n",(0,r.jsx)(n.p,{children:"If you need to process a large dataset at once without setting up a server, you can use vLLM's LLM class.\nFor example, the following code downloads the facebook/opt-125m model from HuggingFace and runs it in vLLM using the default configuration."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'from vllm import LLM\n\n# Initialize the vLLM engine.\nllm = LLM(model="facebook/opt-125m")\n'})}),"\n",(0,r.jsx)(n.p,{children:"After initializing the LLM instance, use the available APIs to perform model inference."}),"\n",(0,r.jsx)(n.h3,{id:"sglang-a-simple-option-for-offline-batch-inference-supplement-material",children:"SGLang: A Simple Option for Offline Batch Inference (Supplement Material)"}),"\n",(0,r.jsxs)(n.p,{children:["For cases where users only want to run batch inference and do not need an HTTP endpoint, SGLang provides a much simpler offline engine API compared to running a full vLLM server. It is particularly suitable for dataset processing, evaluation pipelines, and one-off large-scale inference jobs.\nFor more details and examples, see the official SGLang offline engine documentation:\n",(0,r.jsx)(n.a,{href:"https://docs.sglang.io/basic_usage/offline_engine_api.html",children:"https://docs.sglang.io/basic_usage/offline_engine_api.html"})]}),"\n",(0,r.jsx)(n.h2,{id:"vllm-cli",children:"vLLM CLI"}),"\n",(0,r.jsx)(n.p,{children:"The vllm command-line tool is used to run and manage vLLM models. You can start by viewing the help message with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"vllm --help\n"})}),"\n",(0,r.jsx)(n.p,{children:"Serve - Starts the vLLM OpenAI Compatible API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"vllm serve meta-llama/Llama-2-7b-hf\n"})}),"\n",(0,r.jsx)(n.p,{children:"Chat - Generate chat completions via the running API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# Directly connect to localhost API without arguments\nvllm chat\n\n# Specify API url\nvllm chat --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick chat with a single prompt\nvllm chat --quick "hi"\n'})}),"\n",(0,r.jsx)(n.p,{children:"Complete - Generate text completions based on the given prompt via the running API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# Directly connect to localhost API without arguments\nvllm complete\n\n# Specify API url\nvllm complete --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick complete with a single prompt\nvllm complete --quick "The future of AI is"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["For more CLI command references: visit ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/cli/",children:"https://docs.vllm.ai/en/stable/cli/"}),"."]})]})}function d(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},30416(e,n,l){l.d(n,{R:()=>s,x:()=>o});var t=l(59471);let r={},i=t.createContext(r);function s(e){let n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);