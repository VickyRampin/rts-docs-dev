<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-hpc/ml_ai_hpc/llm_fine_tuning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Fine tune LLMs on HPC | Connecting researchers to computational resources.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://services.rt.nyu.edu/img/NYU.svg"><meta data-rh="true" name="twitter:image" content="https://services.rt.nyu.edu/img/NYU.svg"><meta data-rh="true" property="og:url" content="https://services.rt.nyu.edu/docs/hpc/ml_ai_hpc/llm_fine_tuning/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Fine tune LLMs on HPC | Connecting researchers to computational resources."><meta data-rh="true" name="description" content="Model and Dataset Selection Rationale"><meta data-rh="true" property="og:description" content="Model and Dataset Selection Rationale"><link data-rh="true" rel="icon" href="/img/NYU.ico"><link data-rh="true" rel="canonical" href="https://services.rt.nyu.edu/docs/hpc/ml_ai_hpc/llm_fine_tuning/"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/docs/hpc/ml_ai_hpc/llm_fine_tuning/" hreflang="en"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/docs/hpc/ml_ai_hpc/llm_fine_tuning/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Fine tune LLMs on HPC","item":"https://services.rt.nyu.edu/docs/hpc/ml_ai_hpc/llm_fine_tuning"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Connecting researchers to computational resources. RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Connecting researchers to computational resources. Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2DXB3BDH70"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2DXB3BDH70",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.f829a653.css">
<script src="/assets/js/runtime~main.1423adea.js" defer="defer"></script>
<script src="/assets/js/main.1b6e9861.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/NYU.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_TUNJ" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/NYU.svg" alt="NYU torch logo" class="themedComponent_adik themedComponent--light_G91j"><img src="/img/NYU.svg" alt="NYU torch logo" class="themedComponent_adik themedComponent--dark_ZN7y"></div><b class="navbar__title text--truncate">Research Technology Services</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/genai/getting_started/intro/">Pythia</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/hpc/getting_started/intro/">HPC</a><a class="navbar__item navbar__link" href="/docs/cloud/getting_started/intro/">Cloud</a><a class="navbar__item navbar__link" href="/docs/srde/getting_started/intro/">SRDE</a><a href="https://hsrn.nyu.edu/docs/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">HSRN<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a><a class="navbar__item navbar__link" href="/blog/">Announcements</a><div class="toggle_jqKN colorModeToggle_gY5u"><button class="clean-btn toggleButton_lieL toggleButtonDisabled_sjb2" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU lightToggleIcon_PUFc"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU darkToggleIcon_woI4"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU systemToggleIcon_QZRn"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_gKxF"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_necp"><div class="docsWrapper_wAz_"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_QBOB" type="button"></button><div class="docRoot_gn_g"><aside class="theme-doc-sidebar-container docSidebarContainer_RCWh"><div class="sidebarViewport_BocM"><div class="sidebar_w3s6"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_EeVB"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/getting_started/intro/"><span title="Getting Started" class="categoryLinkLabel_vKI6">Getting Started</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/connecting_to_hpc/connecting_to_hpc/"><span title="Setup" class="categoryLinkLabel_vKI6">Setup</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/storage/intro_and_data_management/"><span title="Storage &amp; Data transfers" class="categoryLinkLabel_vKI6">Storage &amp; Data transfers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/datasets/intro/"><span title="Datasets" class="categoryLinkLabel_vKI6">Datasets</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/submitting_jobs/slurm_submitting_jobs/"><span title="Submitting jobs" class="categoryLinkLabel_vKI6">Submitting jobs</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/tools_and_software/intro/"><span title="Tools &amp; Software" class="categoryLinkLabel_vKI6">Tools &amp; Software</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/containers/intro/"><span title="Containers" class="categoryLinkLabel_vKI6">Containers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/hpc/ml_ai_hpc/intro/"><span title="ML/AI on HPC" class="categoryLinkLabel_vKI6">ML/AI on HPC</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/hpc/ml_ai_hpc/intro/"><span title="Machine Learning (ML) and Artificial Intelligence (AI) on HPC" class="linkLabel_BOGb">Machine Learning (ML) and Artificial Intelligence (AI) on HPC</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/hpc/ml_ai_hpc/pytorch_intro/"><span title="Single-GPU Training with PyTorch" class="linkLabel_BOGb">Single-GPU Training with PyTorch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/hpc/ml_ai_hpc/pytorch_dpp/"><span title="Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)" class="linkLabel_BOGb">Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/hpc/ml_ai_hpc/tensorflow/"><span title="TensorFlow" class="linkLabel_BOGb">TensorFlow</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/hpc/ml_ai_hpc/llm_fine_tuning/"><span title="Fine tune LLMs on HPC" class="linkLabel_BOGb">Fine tune LLMs on HPC</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/hpc/ml_ai_hpc/LLM Inference/run_hf_model/"><span title="LLM Inference" class="categoryLinkLabel_vKI6">LLM Inference</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/ood/ood_intro/"><span title="Open OnDemand (OOD)" class="categoryLinkLabel_vKI6">Open OnDemand (OOD)</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/hpc/spec_sheet/"><span title="Torch Spec Sheet" class="linkLabel_BOGb">Torch Spec Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/hpc/system_status/"><span title="Greene System Status" class="linkLabel_BOGb">Greene System Status</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/tutorial_intro_shell_hpc/intro/"><span title="Tutorial: Intro to Shell for HPC" class="categoryLinkLabel_vKI6">Tutorial: Intro to Shell for HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/tutorial_intro_hpc/intro_hpc/"><span title="Tutorial: Intro to HPC" class="categoryLinkLabel_vKI6">Tutorial: Intro to HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/hpc/support/support/"><span title="Support" class="categoryLinkLabel_vKI6">Support</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_M8Os"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_dZwW"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_cg7U"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_l9oI"><div class="docItemContainer_ENRc"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_N0Nx" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_CnUo"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">ML/AI on HPC</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Fine tune LLMs on HPC</span></li></ul></nav><div class="tocCollapsible_lcDd theme-doc-toc-mobile tocMobile_o_ql"><button type="button" class="clean-btn tocCollapsibleButton_A397">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Fine tune LLMs on HPC</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="model-and-dataset-selection-rationale">Model and Dataset Selection Rationale<a href="#model-and-dataset-selection-rationale" class="hash-link" aria-label="Direct link to Model and Dataset Selection Rationale" title="Direct link to Model and Dataset Selection Rationale" translate="no">​</a></h2>
<table><thead><tr><th>Component</th><th>Configuration</th></tr></thead><tbody><tr><td>Base Model</td><td><code>google/gemma-3-4b-pt</code> (pretrained)</td></tr><tr><td>Comparison Model</td><td><code>google/gemma-3-4b-it</code> (instruction-tuned)</td></tr><tr><td>Dataset</td><td><code>timdettmers/openassistant-guanaco</code></td></tr><tr><td>Justification</td><td>Using Gemma-3 allows direct comparison between base pretrained, our LoRA fine-tuned, and official instruction-tuned variants. The OpenAssistant Guanaco dataset provides high-quality instruction-following examples.</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="dataset-overview">Dataset Overview<a href="#dataset-overview" class="hash-link" aria-label="Direct link to Dataset Overview" title="Direct link to Dataset Overview" translate="no">​</a></h3>
<p>The <code>timdettmers/openassistant-guanaco</code> dataset is a high-quality instruction-following dataset containing conversational exchanges between humans and AI assistants. It includes diverse question-answer pairs covering topics like creative writing, problem-solving, factual queries, and technical explanations. The dataset is specifically designed to train models to follow instructions and provide helpful, harmless, and honest responses.</p>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="fine-tuning-benefits">Fine-tuning Benefits<a href="#fine-tuning-benefits" class="hash-link" aria-label="Direct link to Fine-tuning Benefits" title="Direct link to Fine-tuning Benefits" translate="no">​</a></h3>
<p>Fine-tuning Gemma-3-4B-PT on this dataset significantly improves the model&#x27;s ability to:</p>
<ul>
<li class=""><strong>Follow complex instructions</strong>: Better understanding of multi-step requests and nuanced prompts</li>
<li class=""><strong>Maintain conversational context</strong>: Improved coherence in longer dialogues</li>
<li class=""><strong>Provide structured responses</strong>: Enhanced ability to format answers appropriately</li>
<li class=""><strong>Reduce harmful outputs</strong>: Training on carefully curated data reduces problematic responses</li>
</ul>
<p>By using Gemma-3, we can compare three variants:</p>
<ol>
<li class=""><strong>Base pretrained</strong> (<code>google/gemma-3-4b-pt</code>) - Raw language modeling capabilities</li>
<li class=""><strong>Our LoRA fine-tuned</strong> - Custom instruction following on Guanaco dataset</li>
<li class=""><strong>Official instruction-tuned</strong> (<code>google/gemma-3-4b-it</code>) - Google&#x27;s instruction tuning baseline</li>
</ol>
<p>This provides a comprehensive evaluation of fine-tuning effectiveness.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="three-way-model-comparison">Three-Way Model Comparison<a href="#three-way-model-comparison" class="hash-link" aria-label="Direct link to Three-Way Model Comparison" title="Direct link to Three-Way Model Comparison" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="example-prompt-explain-quantum-computing-in-simple-terms-for-a-beginner">Example Prompt: &quot;Explain quantum computing in simple terms for a beginner&quot;<a href="#example-prompt-explain-quantum-computing-in-simple-terms-for-a-beginner" class="hash-link" aria-label="Direct link to Example Prompt: &quot;Explain quantum computing in simple terms for a beginner&quot;" title="Direct link to Example Prompt: &quot;Explain quantum computing in simple terms for a beginner&quot;" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_wKjT" id="base-pretrained-model-googlegemma-3-4b-pt">Base Pretrained Model (<code>google/gemma-3-4b-pt</code>):<a href="#base-pretrained-model-googlegemma-3-4b-pt" class="hash-link" aria-label="Direct link to base-pretrained-model-googlegemma-3-4b-pt" title="Direct link to base-pretrained-model-googlegemma-3-4b-pt" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-text codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Quantum computing uses quantum bits or qubits instead of classical bits. Qubits can be in superposition states and can be entangled. This allows quantum computers to perform certain calculations exponentially faster than classical computers for specific problems like factoring large numbers and simulating quantum systems.</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_wKjT" id="our-lora-fine-tuned-model-gemma-3-4b-pt--lora-on-guanaco">Our LoRA Fine-tuned Model (Gemma-3-4B-PT + LoRA on Guanaco):<a href="#our-lora-fine-tuned-model-gemma-3-4b-pt--lora-on-guanaco" class="hash-link" aria-label="Direct link to Our LoRA Fine-tuned Model (Gemma-3-4B-PT + LoRA on Guanaco):" title="Direct link to Our LoRA Fine-tuned Model (Gemma-3-4B-PT + LoRA on Guanaco):" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-text codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Great question! Let me break down quantum computing in simple terms:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Think of it like this**: Regular computers use bits that are either 0 or 1 (like light switches that are either ON or OFF). Quantum computers use &quot;quantum bits&quot; or &quot;qubits&quot; that can be 0, 1, or both at the same time - imagine a coin that&#x27;s spinning in the air, being both heads and tails until it lands.</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Why this matters**: This allows quantum computers to:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Try many solutions to a problem simultaneously</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Solve certain types of problems much faster than regular computers</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Handle complex calculations that would take classical computers thousands of years</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Real applications**: Password breaking, drug discovery, weather prediction, and financial modeling.</span><br></span></code></pre></div></div>
<p>The fine-tuned model provides clearer explanations with better structure, analogies, and practical context!</p>
<h4 class="anchor anchorTargetStickyNavbar_wKjT" id="official-instruction-tuned-model-googlegemma-3-4b-it">Official Instruction-tuned Model (<code>google/gemma-3-4b-it</code>):<a href="#official-instruction-tuned-model-googlegemma-3-4b-it" class="hash-link" aria-label="Direct link to official-instruction-tuned-model-googlegemma-3-4b-it" title="Direct link to official-instruction-tuned-model-googlegemma-3-4b-it" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-text codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Quantum computing is a revolutionary approach to computation that leverages quantum mechanics principles. Here&#x27;s a beginner-friendly explanation:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Basic Concept**: While classical computers use bits (0 or 1), quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously through &quot;superposition.&quot;</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Key Advantages**:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Parallel processing of multiple possibilities</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Exponential speedup for specific problem types</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">- Superior performance in cryptography, optimization, and simulation</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">**Current Applications**: Drug discovery, financial modeling, cryptography, and artificial intelligence research.</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>This comparison demonstrates how our custom fine-tuning can achieve similar or better instruction-following capabilities compared to the official instruction-tuned variant.</p>
<hr>
<div class="theme-admonition theme-admonition-tip admonition_nT0D alert alert--success"><div class="admonitionHeading_v1P0"><span class="admonitionIcon_Vpqq"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_kdNk"><p>Complete scripts used are available here: <a href="https://github.com/NYU-RTS/rts-docs-examples/tree/main/hpc/llm_fine_tuning" target="_blank" rel="noopener noreferrer" class="">https://github.com/NYU-RTS/rts-docs-examples/tree/main/hpc/llm_fine_tuning</a></p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="system-environment-setup">System Environment Setup<a href="#system-environment-setup" class="hash-link" aria-label="Direct link to System Environment Setup" title="Direct link to System Environment Setup" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="singularity-container--overlay-configuration">Singularity Container &amp; Overlay Configuration<a href="#singularity-container--overlay-configuration" class="hash-link" aria-label="Direct link to Singularity Container &amp; Overlay Configuration" title="Direct link to Singularity Container &amp; Overlay Configuration" translate="no">​</a></h3>
<table><thead><tr><th>Component</th><th>Configuration</th></tr></thead><tbody><tr><td>Singularity Image</td><td><code>/scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif</code></td></tr><tr><td>Overlay</td><td>Created using <code>singularity overlay create --size 25000 overlay-25GB-conda.ext3</code></td></tr><tr><td>Conda Path</td><td><code>/ext3/miniconda3</code> within overlay</td></tr><tr><td>Singularity Shell Command</td><td>See below</td></tr></tbody></table>
<div class="language-bash codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-bash codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">singularity shell </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--nv</span><span class="token plain"> </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">  </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--overlay</span><span class="token plain"> /scratch/</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&lt;</span><span class="token plain">NetID</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;</span><span class="token plain">/fine-tune/overlay-25GB-conda.ext3:rw </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">  /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="python-environment-and-dependency-installation">Python Environment and Dependency Installation<a href="#python-environment-and-dependency-installation" class="hash-link" aria-label="Direct link to Python Environment and Dependency Installation" title="Direct link to Python Environment and Dependency Installation" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-bash codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token function" style="color:hsl(221, 87%, 60%)">bash</span><span class="token plain"> Miniconda3-latest-Linux-x86_64.sh </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-b</span><span class="token plain"> </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-p</span><span class="token plain"> /ext3/miniconda3</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">source</span><span class="token plain"> /ext3/miniconda3/bin/activate</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">pip </span><span class="token function" style="color:hsl(221, 87%, 60%)">install</span><span class="token plain"> torch transformers datasets accelerate peft trl</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="model-cache-configuration-for-hugging-face">Model Cache Configuration for Hugging Face<a href="#model-cache-configuration-for-hugging-face" class="hash-link" aria-label="Direct link to Model Cache Configuration for Hugging Face" title="Direct link to Model Cache Configuration for Hugging Face" translate="no">​</a></h3>
<p>To avoid exceeding home directory quotas during large model downloads:</p>
<div class="language-bash codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-bash codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:hsl(221, 87%, 60%)">HF_HOME</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">/scratch/</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&lt;</span><span class="token plain">NetID</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;</span><span class="token plain">/.cache/huggingface</span><br></span></code></pre></div></div>
<p>Ensure this is set both interactively and within sbatch scripts.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="operational-troubleshooting-common-errors-and-recommended-fixes">Operational Troubleshooting: Common Errors and Recommended Fixes<a href="#operational-troubleshooting-common-errors-and-recommended-fixes" class="hash-link" aria-label="Direct link to Operational Troubleshooting: Common Errors and Recommended Fixes" title="Direct link to Operational Troubleshooting: Common Errors and Recommended Fixes" translate="no">​</a></h2>
<p>This section provides a comprehensive overview of all environment-related issues encountered during the fine-tuning of <code>google/gemma-3-4b-pt</code> on the NYU Torch HPC cluster. Each entry includes the error symptom, root cause, and resolution strategy, categorized for clarity.</p>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="1-filesystem-and-path-setup-issues">1. Filesystem and Path Setup Issues<a href="#1-filesystem-and-path-setup-issues" class="hash-link" aria-label="Direct link to 1. Filesystem and Path Setup Issues" title="Direct link to 1. Filesystem and Path Setup Issues" translate="no">​</a></h3>
<table><thead><tr><th>Problem</th><th>Symptom</th><th>Cause</th><th>Resolution</th></tr></thead><tbody><tr><td>Incorrect overlay filename</td><td>No such file: <code>overlay-50GB-500K.ext3.gz</code></td><td>The filename was incorrectly assumed</td><td>Use <code>ls /scratch/work/public/overlay-fs-ext3/</code> to verify the correct file: <code>overlay-50G-10M.ext3.gz</code></td></tr><tr><td>Compressed overlay used directly</td><td><code>FATAL: while loading overlay images...</code></td><td>Attempted to use <code>.gz</code> file directly with Singularity</td><td>Run <code>gunzip overlay-50G-10M.ext3.gz</code> before using the file</td></tr><tr><td>Overlay missing in working directory</td><td>sbatch cannot find the overlay file</td><td>Overlay not copied to the training directory</td><td>Ensure the overlay file is placed in <code>/scratch/&lt;NetID&gt;/fine-tune/</code> where sbatch accesses it</td></tr><tr><td>Invalid overlay structure</td><td><code>FATAL: could not create upper dir</code></td><td>Overlay created via <code>fallocate</code> + <code>mkfs.ext3</code>, missing necessary internal structure</td><td>Always use <code>singularity overlay create --size 25000</code> to create overlays</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="2-container-runtime-and-overlay-mounting-errors">2. Container Runtime and Overlay Mounting Errors<a href="#2-container-runtime-and-overlay-mounting-errors" class="hash-link" aria-label="Direct link to 2. Container Runtime and Overlay Mounting Errors" title="Direct link to 2. Container Runtime and Overlay Mounting Errors" translate="no">​</a></h3>
<table><thead><tr><th>Problem</th><th>Symptom</th><th>Cause</th><th>Resolution</th></tr></thead><tbody><tr><td>GPU warning on login node</td><td><code>WARNING: Could not find any nv files</code></td><td><code>--nv</code> flag used outside GPU-enabled session</td><td>Ignore the warning, or only use <code>--nv</code> within a <code>srun --gres=gpu:1</code> session</td></tr><tr><td>Overlay locked by another process</td><td><code>overlay in use by another process</code></td><td>An interactive container shell using the overlay was still active</td><td>Run <code>lsof</code> or <code>ps aux</code> and terminate blocking process</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="3-python-package-installation-and-environment-setup-errors">3. Python Package Installation and Environment Setup Errors<a href="#3-python-package-installation-and-environment-setup-errors" class="hash-link" aria-label="Direct link to 3. Python Package Installation and Environment Setup Errors" title="Direct link to 3. Python Package Installation and Environment Setup Errors" translate="no">​</a></h3>
<table><thead><tr><th>Problem</th><th>Symptom</th><th>Cause</th><th>Resolution</th></tr></thead><tbody><tr><td><code>which pip</code> returns <code>Illegal option --</code></td><td>Unexpected error when checking pip</td><td>Uses <code>/usr/bin/which</code> instead of Bash built-in</td><td>Use <code>command -v pip</code> or simply run <code>pip --version</code></td></tr><tr><td><code>xformers</code> install fails due to missing torch</td><td><code>No module named torch</code> during install</td><td>PyTorch not installed before building <code>xformers</code></td><td>Install torch first: <code>pip install torch</code>, then <code>pip install xformers</code></td></tr><tr><td>Missing <code>transformers</code> in sbatch</td><td><code>ImportError: No module named transformers</code></td><td>Conda not activated in job script</td><td>Add <code>source /ext3/miniconda3/bin/activate</code> before executing the training script</td></tr><tr><td>Installed pip packages not found</td><td>Training job fails to locate modules</td><td>pip used outside overlay context</td><td>Only install packages while the overlay is mounted with <code>:rw</code> in an active container session</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="4-disk-quota-and-cache-management-issues">4. Disk Quota and Cache Management Issues<a href="#4-disk-quota-and-cache-management-issues" class="hash-link" aria-label="Direct link to 4. Disk Quota and Cache Management Issues" title="Direct link to 4. Disk Quota and Cache Management Issues" translate="no">​</a></h3>
<table><thead><tr><th>Problem</th><th>Symptom</th><th>Cause</th><th>Resolution</th></tr></thead><tbody><tr><td>Quota exceeded on home</td><td><code>OSError: [Errno 122] Disk quota exceeded: ~/.cache/huggingface</code></td><td>Default HuggingFace cache path inside <code>/home</code></td><td>Set <code>HF_HOME=/scratch/$USER/.cache/huggingface</code></td></tr><tr><td>Cache redownloading on each sbatch</td><td>Hugging Face cache not shared</td><td><code>HF_HOME</code> not consistently defined</td><td>Persist and reuse the same <code>HF_HOME</code> path across runs</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="5-slurm-job-submission-and-runtime-failures">5. Slurm Job Submission and Runtime Failures<a href="#5-slurm-job-submission-and-runtime-failures" class="hash-link" aria-label="Direct link to 5. Slurm Job Submission and Runtime Failures" title="Direct link to 5. Slurm Job Submission and Runtime Failures" translate="no">​</a></h3>
<table><thead><tr><th>Problem</th><th>Symptom</th><th>Cause</th><th>Resolution</th></tr></thead><tbody><tr><td>Invalid Slurm account</td><td><code>sbatch: Invalid account</code></td><td><code>--account</code> flag not set or invalid</td><td>Use <code>--account=pr_100_tandon_priority</code></td></tr><tr><td>Conda environment not recognized</td><td><code>No module named transformers</code></td><td>Activation missing in sbatch</td><td>Add <code>source /ext3/miniconda3/bin/activate</code> in sbatch</td></tr><tr><td>Overlay not found during job</td><td>sbatch fails to locate file</td><td>Overlay not placed in expected directory</td><td>Ensure all relevant files are in <code>/scratch/&lt;NetID&gt;/fine-tune/</code> or update paths accordingly</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="recommended-best-practices-for-stable-execution">Recommended Best Practices for Stable Execution<a href="#recommended-best-practices-for-stable-execution" class="hash-link" aria-label="Direct link to Recommended Best Practices for Stable Execution" title="Direct link to Recommended Best Practices for Stable Execution" translate="no">​</a></h2>
<table><thead><tr><th>Recommendation</th><th>Rationale</th></tr></thead><tbody><tr><td>Use <code>singularity overlay create</code> for overlay creation</td><td>Ensures <code>upper/</code> and <code>work/</code> directories are properly set up</td></tr><tr><td>Install pip packages only after mounting overlay</td><td>Ensures packages persist and are isolated inside the overlay</td></tr><tr><td>Activate Conda explicitly in sbatch</td><td>Slurm jobs do not inherit interactive shell environments</td></tr><tr><td>Set <code>HF_HOME</code> to <code>/scratch</code></td><td>Prevents hitting disk quota limits in home directories</td></tr><tr><td>Avoid <code>return_tensors=&quot;pt&quot;</code> in tokenizer mapping</td><td>Leads to shape mismatch errors in batched training</td></tr><tr><td>Use subset sampling (e.g., <code>train[:1%]</code>) for testing</td><td>Minimizes resource consumption and enables fast debugging</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="lora-configuration-parameters">LoRA Configuration Parameters<a href="#lora-configuration-parameters" class="hash-link" aria-label="Direct link to LoRA Configuration Parameters" title="Direct link to LoRA Configuration Parameters" translate="no">​</a></h2>
<p>LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning large models with reduced computational cost. It adapts the model&#x27;s layers by adding low-rank matrices while maintaining the original model&#x27;s parameters. This enables efficient training with fewer resources.</p>
<p>Learn more about LoRA <a href="https://huggingface.co/learn/llm-course/en/chapter11/4" target="_blank" rel="noopener noreferrer" class="">here</a>.</p>
<p>Here are the configuration parameters used for LoRA in this fine-tuning setup:</p>
<div class="language-python codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-python codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">peft_config </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> LoraConfig</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    r</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">8</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    lora_alpha</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">16</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    target_modules</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">[</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;q_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;k_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;v_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;o_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">]</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    lora_dropout</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">0.05</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    bias</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;none&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    task_type</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">TaskType</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token plain">CAUSAL_LM</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="sbatch-job-script-for-model-training">sbatch Job Script for Model Training<a href="#sbatch-job-script-for-model-training" class="hash-link" aria-label="Direct link to sbatch Job Script for Model Training" title="Direct link to sbatch Job Script for Model Training" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="training-script-train_gemma3py"><strong>Training Script: <code>train_gemma3.py</code></strong><a href="#training-script-train_gemma3py" class="hash-link" aria-label="Direct link to training-script-train_gemma3py" title="Direct link to training-script-train_gemma3py" translate="no">​</a></h3>
<p>The complete training script is available <a href="https://github.com/NYU-RTS/rts-docs-examples/tree/main/hpc/llm_fine_tuning" target="_blank" rel="noopener noreferrer" class="">here</a>. Below are the key configuration snippets:</p>
<p><strong>Model and Dataset Configuration:</strong></p>
<div class="language-python codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-python codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token comment" style="color:hsl(230, 4%, 64%)"># Model and dataset configuration</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">model_name </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;google/gemma-3-4b-pt&quot;</span><span class="token plain">  </span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Base pretrained model</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">dataset_name </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;timdettmers/openassistant-guanaco&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">output_dir </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;./gemma3_output&quot;</span><br></span></code></pre></div></div>
<p><strong>LoRA Configuration:</strong></p>
<div class="language-python codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-python codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token comment" style="color:hsl(230, 4%, 64%)"># LoRA configuration</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">peft_config </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> LoraConfig</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    r</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">8</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    lora_alpha</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">16</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    target_modules</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">[</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;q_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;k_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;v_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;o_proj&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">]</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    lora_dropout</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">0.05</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    bias</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;none&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    task_type</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">TaskType</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token plain">CAUSAL_LM</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><br></span></code></pre></div></div>
<p><strong>Training Arguments:</strong></p>
<div class="language-python codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-python codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token comment" style="color:hsl(230, 4%, 64%)"># Training arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">training_args </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> TrainingArguments</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    output_dir</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">output_dir</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    per_device_train_batch_size</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">4</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    gradient_accumulation_steps</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">4</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    num_train_epochs</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">1</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    learning_rate</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">2e-4</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    fp16</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token boolean" style="color:hsl(35, 99%, 36%)">True</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    logging_steps</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">10</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    save_steps</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">50</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    save_total_limit</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">2</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    remove_unused_columns</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token boolean" style="color:hsl(35, 99%, 36%)">False</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    dataloader_pin_memory</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token boolean" style="color:hsl(35, 99%, 36%)">False</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="sbatch-script"><strong>sbatch Script</strong><a href="#sbatch-script" class="hash-link" aria-label="Direct link to sbatch-script" title="Direct link to sbatch-script" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-bash codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token shebang important" style="color:hsl(230, 8%, 24%);font-weight:bold">#!/bin/bash</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --job-name=gemma3-finetune</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --nodes=1</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --ntasks-per-node=1</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --cpus-per-task=4</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mem=40GB</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --gres=gpu:1</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --time=12:00:00</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --output=/scratch/&lt;NetID&gt;/fine-tune/gemma3_train_%j.out</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --error=/scratch/&lt;NetID&gt;/fine-tune/gemma3_train_%j.err</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mail-type=END,FAIL</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mail-user=&lt;NetID&gt;@nyu.edu</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:hsl(221, 87%, 60%)">HF_HOME</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">/scratch/</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&lt;</span><span class="token plain">NetID</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;</span><span class="token plain">/.cache/huggingface</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">singularity </span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--nv</span><span class="token plain"> </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">  </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--overlay</span><span class="token plain"> /scratch/</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&lt;</span><span class="token plain">NetID</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;</span><span class="token plain">/fine-tune/overlay-25GB-conda.ext3:rw </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">  /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">  /bin/bash </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-c</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">    source /ext3/miniconda3/bin/activate</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">    cd /scratch/&lt;NetID&gt;/fine-tune</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">    python train_gemma3.py</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="generated-output-artifacts">Generated Output Artifacts<a href="#generated-output-artifacts" class="hash-link" aria-label="Direct link to Generated Output Artifacts" title="Direct link to Generated Output Artifacts" translate="no">​</a></h2>
<table><thead><tr><th>File</th><th>Description</th></tr></thead><tbody><tr><td><code>adapter_model.safetensors</code></td><td>LoRA adapter weights</td></tr><tr><td><code>adapter_config.json</code></td><td>Adapter architecture definition</td></tr><tr><td><code>trainer_state.json</code></td><td>Training metadata</td></tr><tr><td><code>training_args.bin</code></td><td>Saved training configuration</td></tr><tr><td><code>tokenizer_config.json</code>, <code>tokenizer.json</code></td><td>Tokenizer data</td></tr></tbody></table>
<p>Location: <code>/scratch/&lt;NetID&gt;/fine-tune/gemma3_output/checkpoint-13/</code></p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="training-completion-summary">Training Completion Summary<a href="#training-completion-summary" class="hash-link" aria-label="Direct link to Training Completion Summary" title="Direct link to Training Completion Summary" translate="no">​</a></h2>
<table><thead><tr><th>Epochs</th><th>Steps</th><th>Status</th></tr></thead><tbody><tr><td>1</td><td>13</td><td>Completed successfully</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_AAw7"><a href="https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/06_llm_fine_tuning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_OZ0G" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_RgrK"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/hpc/ml_ai_hpc/tensorflow/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">TensorFlow</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/hpc/ml_ai_hpc/LLM Inference/run_hf_model/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Run a Hugging Face model</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_Py9d thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#model-and-dataset-selection-rationale" class="table-of-contents__link toc-highlight">Model and Dataset Selection Rationale</a><ul><li><a href="#dataset-overview" class="table-of-contents__link toc-highlight">Dataset Overview</a></li><li><a href="#fine-tuning-benefits" class="table-of-contents__link toc-highlight">Fine-tuning Benefits</a></li></ul></li><li><a href="#three-way-model-comparison" class="table-of-contents__link toc-highlight">Three-Way Model Comparison</a><ul><li><a href="#example-prompt-explain-quantum-computing-in-simple-terms-for-a-beginner" class="table-of-contents__link toc-highlight">Example Prompt: &quot;Explain quantum computing in simple terms for a beginner&quot;</a></li></ul></li><li><a href="#system-environment-setup" class="table-of-contents__link toc-highlight">System Environment Setup</a><ul><li><a href="#singularity-container--overlay-configuration" class="table-of-contents__link toc-highlight">Singularity Container &amp; Overlay Configuration</a></li><li><a href="#python-environment-and-dependency-installation" class="table-of-contents__link toc-highlight">Python Environment and Dependency Installation</a></li><li><a href="#model-cache-configuration-for-hugging-face" class="table-of-contents__link toc-highlight">Model Cache Configuration for Hugging Face</a></li></ul></li><li><a href="#operational-troubleshooting-common-errors-and-recommended-fixes" class="table-of-contents__link toc-highlight">Operational Troubleshooting: Common Errors and Recommended Fixes</a><ul><li><a href="#1-filesystem-and-path-setup-issues" class="table-of-contents__link toc-highlight">1. Filesystem and Path Setup Issues</a></li><li><a href="#2-container-runtime-and-overlay-mounting-errors" class="table-of-contents__link toc-highlight">2. Container Runtime and Overlay Mounting Errors</a></li><li><a href="#3-python-package-installation-and-environment-setup-errors" class="table-of-contents__link toc-highlight">3. Python Package Installation and Environment Setup Errors</a></li><li><a href="#4-disk-quota-and-cache-management-issues" class="table-of-contents__link toc-highlight">4. Disk Quota and Cache Management Issues</a></li><li><a href="#5-slurm-job-submission-and-runtime-failures" class="table-of-contents__link toc-highlight">5. Slurm Job Submission and Runtime Failures</a></li></ul></li><li><a href="#recommended-best-practices-for-stable-execution" class="table-of-contents__link toc-highlight">Recommended Best Practices for Stable Execution</a></li><li><a href="#lora-configuration-parameters" class="table-of-contents__link toc-highlight">LoRA Configuration Parameters</a></li><li><a href="#sbatch-job-script-for-model-training" class="table-of-contents__link toc-highlight">sbatch Job Script for Model Training</a><ul><li><a href="#training-script-train_gemma3py" class="table-of-contents__link toc-highlight"><strong>Training Script: <code>train_gemma3.py</code></strong></a></li><li><a href="#sbatch-script" class="table-of-contents__link toc-highlight"><strong>sbatch Script</strong></a></li></ul></li><li><a href="#generated-output-artifacts" class="table-of-contents__link toc-highlight">Generated Output Artifacts</a></li><li><a href="#training-completion-summary" class="table-of-contents__link toc-highlight">Training Completion Summary</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:hpc@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HPC support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:hsrn-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HSRN support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:research-cloud-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Research Cloud support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:srde-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Secure Research Data Environment support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email general GenAI support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-research-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email GenAI for research support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://guides.nyu.edu/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NYU Libraries Research Guides<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://sites.google.com/nyu.edu/forc-camp/home" target="_blank" rel="noopener noreferrer" class="footer__link-item">FORC camp<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.google.com/forms/d/e/1FAIpQLSeHnmkPdR_IvWnT6a7U_V3RpfmQrpS8hjxI11FNnsZMlrBa4g/viewform" target="_blank" rel="noopener noreferrer" class="footer__link-item">Feedback</a></li><li class="footer__item"><a class="footer__link-item" href="/blog/">Announcements</a></li><li class="footer__item"><a href="https://github.com/NYU-RTS/rts-docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Made with 💜 in NYC!</div></div></div></footer></div>
</body>
</html>